
@article{kipf_semisupervisedclassificationgraph_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.02907},
  primaryClass = {cs, stat},
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  journal = {arXiv:1609.02907 [cs, stat]},
  author = {Kipf, Thomas N. and Welling, Max},
  month = sep,
  year = {2016},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Learning},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\CP7ESRKA\\kipf_welling_2016_semi-supervised_classification_with_graph_convolutional_networks.pdf;C:\\Users\\yunsh782\\zotero\\storage\\D37FBXJI\\kipf_welling_2016_semi-supervised_classification_with_graph_convolutional_networks.pdf}
}

@article{biarnes_metaguivmdinterface_2012,
  title = {{{METAGUI}}. {{A VMD}} Interface for Analyzing Metadynamics and Molecular Dynamics Simulations},
  volume = {183},
  issn = {0010-4655},
  doi = {10.1016/j.cpc.2011.08.020},
  abstract = {We present a new computational tool, METAGUI, which extends the VMD program with a graphical user interface that allows constructing a thermodynamic and kinetic model of a given process simulated by large-scale molecular dynamics. The tool is specially designed for analyzing metadynamics based simulations. The huge amount of diverse structures generated during such a simulation is partitioned into a set of microstates (i.e. structures with similar values of the collective variables). Their relative free energies are then computed by a weighted-histogram procedure and the most relevant free energy wells are identified by diagonalization of the rate matrix followed by a commitor analysis. All this procedure leads to a convenient representation of the metastable states and long-time kinetics of the system which can be compared with experimental data. The tool allows to seamlessly switch between a collective variables space representation of microstates and their atomic structure representation, which greatly facilitates the set-up and analysis of molecular dynamics simulations. METAGUI is based on the output format of the PLUMED plugin, making it compatible with a number of different molecular dynamics packages like AMBER, NAMD, GROMACS and several others. The METAGUI source files can be downloaded from the PLUMED web site (http://www.plumed-code.org).
Program summary
Program title: METAGUI Catalogue identifier: AEKH\_v1\_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEKH\_v1\_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: GNU General Public License version 3 No. of lines in distributed program, including test data, etc.: 117\,545 No. of bytes in distributed program, including test data, etc.: 8\,516\,203 Distribution format: tar.gz Programming language: TK/TCL, Fortran Computer: Any computer with a VMD installation and capable of running an executable produced by a gfortran compiler Operating system: Linux, Unix OS-es RAM: 1\,073\,741\,824 bytes Classification: 23 External routines: A VMD installation (http://www.ks.uiuc.edu/Research/vmd/) Nature of problem: Extract thermodynamic data and build a kinetic model of a given process simulated by metadynamics or molecular dynamics simulations, and provide this information on a dual representation that allows navigating and exploring the molecular structures corresponding to each point along the multi-dimensional free energy hypersurface. Solution method: Graphical-user interface linked to VMD that1.clusterizes the simulation trajectories in the space of a set of collective variables and assigns each frame to a given microstate,2.determines the free energy of each microstate by a weighted histogram analysis method, and3.identifies the most relevant free energy wells (kinetic basins) by diagonalization of the rate matrix followed by a commitor analysis. Restrictions: Input format files compatible with PLUMED and all the MD engines supported by PLUMED and VMD. Running time: A few minutes.},
  number = {1},
  journal = {Computer Physics Communications},
  author = {Biarn\'es, Xevi and Pietrucci, Fabio and Marinelli, Fabrizio and Laio, Alessandro},
  month = jan,
  year = {2012},
  keywords = {Bias exchange,Kinetics,Metadynamics,Molecular dynamics simulation,PLUMED,Thermodynamics,VMD},
  pages = {203-211},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\NEL5E3MQ\\biarnés_et_al_2012_metagui.pdf}
}

@article{lubbers_hierarchicalmodelingmolecular_2018,
  title = {Hierarchical Modeling of Molecular Energies Using a Deep Neural Network},
  volume = {148},
  issn = {0021-9606, 1089-7690},
  doi = {10.1063/1.5011181},
  language = {en},
  number = {24},
  journal = {The Journal of Chemical Physics},
  author = {Lubbers, Nicholas and Smith, Justin S. and Barros, Kipton},
  month = jun,
  year = {2018},
  pages = {241715},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\LSQZBK6Y\\lubbers_et_al_2018_hierarchical_modeling_of_molecular_energies_using_a_deep_neural_network.pdf}
}

@article{bartok_gaussianapproximationpotentials_2010,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0910.1019},
  title = {Gaussian {{Approximation Potentials}}: The Accuracy of Quantum Mechanics, without the Electrons},
  volume = {104},
  issn = {0031-9007, 1079-7114},
  shorttitle = {Gaussian {{Approximation Potentials}}},
  doi = {10.1103/PhysRevLett.104.136403},
  abstract = {We introduce a class of interatomic potential models that can be automatically generated from data consisting of the energies and forces experienced by atoms, derived from quantum mechanical calculations. The resulting model does not have a fixed functional form and hence is capable of modeling complex potential energy landscapes. It is systematically improvable with more data. We apply the method to bulk carbon, silicon and germanium and test it by calculating properties of the crystals at high temperatures. Using the interatomic potential to generate the long molecular dynamics trajectories required for such calculations saves orders of magnitude in computational cost.},
  number = {13},
  journal = {Physical Review Letters},
  author = {Bart\'ok, Albert P. and Payne, Mike C. and Kondor, Risi and Cs\'anyi, G\'abor},
  month = apr,
  year = {2010},
  keywords = {Condensed Matter - Materials Science,Physics - Computational Physics},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\HFVXVTS7\\bartók_et_al_2010_gaussian_approximation_potentials.pdf;C:\\Users\\yunsh782\\zotero\\storage\\TFLQUNFZ\\bartók_et_al_2010_gaussian_approximation_potentials.pdf}
}

@article{schutt_schnetdeeplearning_2018,
  title = {{{SchNet}} \textendash{} {{A}} Deep Learning Architecture for Molecules and Materials},
  volume = {148},
  issn = {0021-9606, 1089-7690},
  doi = {10.1063/1.5019779},
  language = {en},
  number = {24},
  journal = {The Journal of Chemical Physics},
  author = {Sch\"utt, K. T. and Sauceda, H. E. and Kindermans, P.-J. and Tkatchenko, A. and M\"uller, K.-R.},
  month = jun,
  year = {2018},
  pages = {241722},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\53R7MSUG\\schütt_et_al_2018_schnet_–_a_deep_learning_architecture_for_molecules_and_materials.pdf;C:\\Users\\yunsh782\\zotero\\storage\\9CPTCJYD\\schütt_et_al_2018_schnet_–_a_deep_learning_architecture_for_molecules_and_materials.pdf}
}

@article{imbalzano_automaticselectionatomic_2018,
  title = {Automatic Selection of Atomic Fingerprints and Reference Configurations for Machine-Learning Potentials},
  volume = {148},
  issn = {0021-9606},
  doi = {10.1063/1.5024611},
  abstract = {Machine learning of atomic-scale properties is revolutionizing molecular modeling, making it possible to evaluate inter-atomic potentials with first-principles accuracy, at a fraction of the costs. The accuracy, speed, and reliability of machine learning potentials, however, depend strongly on the way atomic configurations are represented, i.e., the choice of descriptors used as input for the machine learning method. The raw Cartesian coordinates are typically transformed in ``fingerprints,'' or ``symmetry functions,'' that are designed to encode, in addition to the structure, important properties of the potential energy surface like its invariances with respect to rotation, translation, and permutation of like atoms. Here we discuss automatic protocols to select a number of fingerprints out of a large pool of candidates, based on the correlations that are intrinsic to the training data. This procedure can greatly simplify the construction of neural network potentials that strike the best balance between accuracy and computational efficiency and has the potential to accelerate by orders of magnitude the evaluation of Gaussian approximation potentials based on the smooth overlap of atomic positions kernel. We present applications to the construction of neural network potentials for water and for an Al\textendash{}Mg\textendash{}Si alloy and to the prediction of the formation energies of small organic molecules using Gaussian process regression.},
  number = {24},
  journal = {The Journal of Chemical Physics},
  author = {Imbalzano, Giulio and Anelli, Andrea and Giofr\'e, Daniele and Klees, Sinja and Behler, J\"org and Ceriotti, Michele},
  month = apr,
  year = {2018},
  pages = {241730},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\IJZLNQ3Q\\imbalzano_et_al_2018_automatic_selection_of_atomic_fingerprints_and_reference_configurations_for.pdf;C:\\Users\\yunsh782\\zotero\\storage\\PSBBQBQW\\imbalzano_et_al_2018_automatic_selection_of_atomic_fingerprints_and_reference_configurations_for.pdf}
}

@article{kamath_neuralnetworksvs_2018,
  title = {Neural Networks vs {{Gaussian}} Process Regression for Representing Potential Energy Surfaces: {{A}} Comparative Study of Fit Quality and Vibrational Spectrum Accuracy},
  volume = {148},
  issn = {0021-9606, 1089-7690},
  shorttitle = {Neural Networks vs {{Gaussian}} Process Regression for Representing Potential Energy Surfaces},
  doi = {10.1063/1.5003074},
  language = {en},
  number = {24},
  journal = {The Journal of Chemical Physics},
  author = {Kamath, Aditya and {Vargas-Hern\'andez}, Rodrigo A. and Krems, Roman V. and Carrington, Tucker and Manzhos, Sergei},
  month = jun,
  year = {2018},
  pages = {241702},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\4QP57E98\\kamath_et_al_2018_neural_networks_vs_gaussian_process_regression_for_representing_potential.pdf}
}

@article{carrasco_multibodyexpansionparticle_2016,
  title = {Multibody Expansion of Particle Interactions: {{How}} Many-Body Is a Particular Element in a Cluster},
  volume = {94},
  issn = {2469-9950, 2469-9969},
  shorttitle = {Multibody Expansion of Particle Interactions},
  doi = {10.1103/PhysRevB.94.075435},
  language = {en},
  number = {7},
  journal = {Physical Review B},
  author = {Carrasco, Sebasti\'an and Varas, Alejandro and Rogan, Jos\'e and Kiwi, Miguel and Valdivia, Juan Alejandro},
  month = aug,
  year = {2016},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\SGCI44QQ\\carrasco_et_al_2016_multibody_expansion_of_particle_interactions.pdf}
}

@article{grisafi_symmetryadaptedmachinelearning_2018,
  title = {Symmetry-{{Adapted Machine Learning}} for {{Tensorial Properties}} of {{Atomistic Systems}}},
  volume = {120},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.120.036002},
  language = {en},
  number = {3},
  journal = {Physical Review Letters},
  author = {Grisafi, Andrea and Wilkins, David M. and Cs\'anyi, G\'abor and Ceriotti, Michele},
  month = jan,
  year = {2018},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\QESXNMQM\\grisafi_et_al_2018_symmetry-adapted_machine_learning_for_tensorial_properties_of_atomistic_systems.pdf;C:\\Users\\yunsh782\\zotero\\storage\\XSHMKZCP\\grisafi_et_al_2018_symmetry-adapted_machine_learning_for_tensorial_properties_of_atomistic_systems.pdf}
}

@article{sprik_initiomoleculardynamics_1996,
  title = {{\emph{Ab}} {\emph{Initio}} Molecular Dynamics Simulation of Liquid Water: {{Comparison}} of Three Gradient-corrected Density Functionals},
  volume = {105},
  issn = {0021-9606, 1089-7690},
  shorttitle = {{\emph{Ab}} {\emph{Initio}} Molecular Dynamics Simulation of Liquid Water},
  doi = {10.1063/1.471957},
  language = {en},
  number = {3},
  journal = {The Journal of Chemical Physics},
  author = {Sprik, Michiel and Hutter, J\"urg and Parrinello, Michele},
  month = jul,
  year = {1996},
  pages = {1142-1152},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\DR4XGK2Z\\sprik_et_al_1996_iab-i_iinitio-i_molecular_dynamics_simulation_of_liquid_water.pdf;C:\\Users\\yunsh782\\zotero\\storage\\MQ2K8954\\sprik_et_al_1996_iab-i_iinitio-i_molecular_dynamics_simulation_of_liquid_water.pdf}
}

@article{cisneros_modelingmolecularinteractions_2016,
  title = {Modeling {{Molecular Interactions}} in {{Water}}: {{From Pairwise}} to {{Many}}-{{Body Potential Energy Functions}}},
  volume = {116},
  issn = {0009-2665, 1520-6890},
  shorttitle = {Modeling {{Molecular Interactions}} in {{Water}}},
  doi = {10.1021/acs.chemrev.5b00644},
  abstract = {Almost 50 years have passed from the first computer simulations of water, and a large number of molecular models have been proposed since then to elucidate the unique behavior of water across different phases. In this article, we review the recent progress in the development of analytical potential energy functions that aim at correctly representing many-body effects. Starting from the many-body expansion of the interaction energy, specific focus is on different classes of potential energy functions built upon a hierarchy of approximations and on their ability to accurately reproduce reference data obtained from state-of-the-art electronic structure calculations and experimental measurements. We show that most recent potential energy functions, which include explicit short-range representations of two-body and three-body effects along with a physically correct description of many-body effects at all distances, predict the properties of water from the gas to the condensed phase with unprecedented accuracy, thus opening the door to the long-sought ``universal model'' capable of describing the behavior of water under different conditions and in different environments.},
  language = {en},
  number = {13},
  journal = {Chemical Reviews},
  author = {Cisneros, Gerardo Andr\'es and Wikfeldt, Kjartan Thor and Ojam\"ae, Lars and Lu, Jibao and Xu, Yao and Torabifard, Hedieh and Bart\'ok, Albert P. and Cs\'anyi, G\'abor and Molinero, Valeria and Paesani, Francesco},
  month = jul,
  year = {2016},
  pages = {7501-7528},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\FXCQP7MP\\cisneros_et_al_2016_modeling_molecular_interactions_in_water.pdf}
}

@article{babin_developmentfirstprinciples_2013,
  title = {Development of a ``{{First Principles}}'' {{Water Potential}} with {{Flexible Monomers}}: {{Dimer Potential Energy Surface}}, {{VRT Spectrum}}, and {{Second Virial Coefficient}}},
  volume = {9},
  issn = {1549-9618, 1549-9626},
  shorttitle = {Development of a ``{{First Principles}}'' {{Water Potential}} with {{Flexible Monomers}}},
  doi = {10.1021/ct400863t},
  abstract = {The development of a ``first principles'' water potential with flexible monomers (MB-pol) for molecular simulations of water systems from gas to condensed phases is described. MB-pol is built upon the many-body expansion of the intermolecular interactions, and the specific focus of this study is on the two-body term (V2B) representing the full-dimensional intermolecular part of the water dimer potential energy surface. V2B is constructed by fitting 40,000 dimer energies calculated at the CCSD(T)/CBS level of theory and imposing the correct asymptotic behavior at long-range as predicted from ``first principles''. The comparison of the calculated vibration-rotation tunneling (VRT) spectrum and second virial coefficient with the corresponding experimental results demonstrates the accuracy of the MB-pol dimer potential energy surface.},
  language = {en},
  number = {12},
  journal = {Journal of Chemical Theory and Computation},
  author = {Babin, Volodymyr and Leforestier, Claude and Paesani, Francesco},
  month = dec,
  year = {2013},
  pages = {5395-5403},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\47CI9PAG\\babin_et_al_2013_development_of_a_“first_principles”_water_potential_with_flexible_monomers.pdf}
}

@article{vanderwilk_convolutionalgaussianprocesses_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.01894},
  primaryClass = {cs, stat},
  title = {Convolutional {{Gaussian Processes}}},
  abstract = {We present a practical way of introducing convolutional structure into Gaussian processes, making them more suited to high-dimensional inputs like images. The main contribution of our work is the construction of an inter-domain inducing point approximation that is well-tailored to the convolutional kernel. This allows us to gain the generalisation benefit of a convolutional kernel, together with fast but accurate posterior inference. We investigate several variations of the convolutional kernel, and apply it to MNIST and CIFAR-10, which have both been known to be challenging for Gaussian processes. We also show how the marginal likelihood can be used to find an optimal weighting between convolutional and RBF kernels to further improve performance. We hope that this illustration of the usefulness of a marginal likelihood will help automate discovering architectures in larger models.},
  journal = {arXiv:1709.01894 [cs, stat]},
  author = {{van der Wilk}, Mark and Rasmussen, Carl Edward and Hensman, James},
  month = sep,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\6M2FD4N2\\van_der_wilk_et_al_2017_convolutional_gaussian_processes.pdf}
}

@article{lee_deepneuralnetworks_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.00165},
  primaryClass = {cs, stat},
  title = {Deep {{Neural Networks}} as {{Gaussian Processes}}},
  abstract = {It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks.},
  journal = {arXiv:1711.00165 [cs, stat]},
  author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and {Sohl-Dickstein}, Jascha},
  month = oct,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\3LCY4G8C\\lee_et_al_2017_deep_neural_networks_as_gaussian_processes.pdf}
}

@misc{poczos_kernelmethods_,
  title = {Kernel {{Methods}}},
  author = {P\'oczos, Barnab\'as},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\LRCLARNK\\kernel_methods.pdf}
}

@misc{kentaoono_deeplearningmolecules_05:28:55UTC,
  type = {Technology},
  title = {Deep Learning for Molecules, Introduction to Chainer Chemistry},
  abstract = {Deep learning for molecules, introduction to chainer chemistry},
  author = {Kenta Oono},
  year = {05:28:55 UTC}
}

@article{nguyen_comparisonpermutationallyinvariant_2018,
  title = {Comparison of Permutationally Invariant Polynomials, Neural Networks, and {{Gaussian}} Approximation Potentials in Representing Water Interactions through Many-Body Expansions},
  volume = {148},
  issn = {0021-9606, 1089-7690},
  doi = {10.1063/1.5024577},
  language = {en},
  number = {24},
  journal = {The Journal of Chemical Physics},
  author = {Nguyen, Thuong T. and Sz\'ekely, Eszter and Imbalzano, Giulio and Behler, J\"org and Cs\'anyi, G\'abor and Ceriotti, Michele and G\"otz, Andreas W. and Paesani, Francesco},
  month = jun,
  year = {2018},
  pages = {241725},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\EFYRIMB2\\nguyen_et_al_2018_comparison_of_permutationally_invariant_polynomials,_neural_networks,_and.pdf}
}

@article{raccuglia_machinelearningassistedmaterialsdiscovery_2016,
  title = {Machine-Learning-Assisted Materials Discovery Using Failed Experiments},
  volume = {533},
  copyright = {2016 Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature17439},
  abstract = {Inorganic\textendash{}organic hybrid materials1,2,3 such as organically templated metal oxides1, metal\textendash{}organic frameworks (MOFs)2 and organohalide perovskites4 have been studied for decades, and hydrothermal and (non-aqueous) solvothermal syntheses have produced thousands of new materials that collectively contain nearly all the metals in the periodic table5,6,7,8,9. Nevertheless, the formation of these compounds is not fully understood, and development of new compounds relies primarily on exploratory syntheses. Simulation- and data-driven approaches (promoted by efforts such as the Materials Genome Initiative10) provide an alternative to experimental trial-and-error. Three major strategies are: simulation-based predictions of physical properties (for example, charge mobility11, photovoltaic properties12, gas adsorption capacity13 or lithium-ion intercalation14) to identify promising target candidates for synthetic efforts11,15; determination of the structure\textendash{}property relationship from large bodies of experimental data16,17, enabled by integration with high-throughput synthesis and measurement tools18; and clustering on the basis of similar crystallographic structure (for example, zeolite structure classification19,20 or gas adsorption properties21). Here we demonstrate an alternative approach that uses machine-learning algorithms trained on reaction data to predict reaction outcomes for the crystallization of templated vanadium selenites. We used information on `dark' reactions\textemdash{}failed or unsuccessful hydrothermal syntheses\textemdash{}collected from archived laboratory notebooks from our laboratory, and added physicochemical property descriptions to the raw notebook information using cheminformatics techniques. We used the resulting data to train a machine-learning model to predict reaction success. When carrying out hydrothermal synthesis experiments using previously untested, commercially available organic building blocks, our machine-learning model outperformed traditional human strategies, and successfully predicted conditions for new organically templated inorganic product formation with a success rate of 89 per cent. Inverting the machine-learning model reveals new hypotheses regarding the conditions for successful product formation.},
  language = {en},
  number = {7601},
  journal = {Nature},
  author = {Raccuglia, Paul and Elbert, Katherine C. and Adler, Philip D. F. and Falk, Casey and Wenny, Malia B. and Mollo, Aurelio and Zeller, Matthias and Friedler, Sorelle A. and Schrier, Joshua and Norquist, Alexander J.},
  month = may,
  year = {2016},
  pages = {73-76},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\568P28U7\\raccuglia_et_al_2016_machine-learning-assisted_materials_discovery_using_failed_experiments.pdf}
}

@article{herr_metadynamicstrainingneural_2018,
  title = {Metadynamics for Training Neural Network Model Chemistries: {{A}} Competitive Assessment},
  volume = {148},
  issn = {0021-9606, 1089-7690},
  shorttitle = {Metadynamics for Training Neural Network Model Chemistries},
  doi = {10.1063/1.5020067},
  language = {en},
  number = {24},
  journal = {The Journal of Chemical Physics},
  author = {Herr, John E. and Yao, Kun and McIntyre, Ryker and Toth, David W. and Parkhill, John},
  month = jun,
  year = {2018},
  pages = {241710},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\UJT3FADA\\herr_et_al_2018_metadynamics_for_training_neural_network_model_chemistries.pdf}
}

@article{hazan_hyperparameteroptimizationspectral_2018,
  title = {{{HYPERPARAMETER OPTIMIZATION}}: {{A SPECTRAL APPROACH}}},
  abstract = {We give a simple, fast algorithm for hyperparameter optimization inspired by techniques from the analysis of Boolean functions. We focus on the high-dimensional regime where the canonical example is training a neural network with a large number of hyperparameters. The algorithm \textemdash{} an iterative application of compressed sensing techniques for orthogonal polynomials \textemdash{} requires only uniform sampling of the hyperparameters and is thus easily parallelizable.},
  language = {en},
  author = {Hazan, Elad and Yuan, Yang and Klivans, Adam},
  year = {2018},
  pages = {18},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\QUEL7LAX\\hazan_et_al_2018_hyperparameter_optimization.pdf}
}

@article{bergstra_algorithmshyperparameteroptimization_,
  title = {Algorithms for {{Hyper}}-{{Parameter Optimization}}},
  abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
  language = {en},
  author = {Bergstra, James S and Bardenet, R\'emi and Bengio, Yoshua and K\'egl, Bal\'azs},
  pages = {9},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\4GIV3PFK\\bergstra_et_al_algorithms_for_hyper-parameter_optimization.pdf}
}

@article{sculley_paceprogressempirical_2018,
  title = {{{ON PACE}}, {{PROGRESS}}, {{AND EMPIRICAL RIGOR}}},
  abstract = {The field of ML is distinguished both by rapid innovation and rapid dissemination of results. While the pace of progress has been extraordinary by any measure, in this paper we explore potential issues that we believe to be arising as a result. In particular, we observe that the rate of empirical advancement may not have been matched by consistent increase in the level of empirical rigor across the field as a whole. This short position paper highlights examples where progress has actually been slowed as a result, offers thoughts on incentive structures currently at play, and gives suggestions as seeds for discussions on productive change.},
  language = {en},
  author = {Sculley, D and Snoek, Jasper and Rahimi, Ali and Wiltschko, Alex},
  year = {2018},
  pages = {4},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\N4UZMACF\\sculley_et_al_2018_on_pace,_progress,_and_empirical_rigor.pdf}
}

@article{ruddigkeit_enumeration166billion_2012,
  title = {Enumeration of 166 {{Billion Organic Small Molecules}} in the {{Chemical Universe Database GDB}}-17},
  volume = {52},
  issn = {1549-9596, 1549-960X},
  doi = {10.1021/ci300415d},
  abstract = {Drug molecules consist of a few tens of atoms connected by covalent bonds. How many such molecules are possible in total and what is their structure? This question is of pressing interest in medicinal chemistry to help solve the problems of drug potency, selectivity, and toxicity and reduce attrition rates by pointing to new molecular series. To better define the unknown chemical space, we have enumerated 166.4 billion molecules of up to 17 atoms of C, N, O, S, and halogens forming the chemical universe database GDB-17, covering a size range containing many drugs and typical for lead compounds. GDB-17 contains millions of isomers of known drugs, including analogs with high shape similarity to the parent drug. Compared to known molecules in PubChem, GDB-17 molecules are much richer in nonaromatic heterocycles, quaternary centers, and stereoisomers, densely populate the third dimension in shape space, and represent many more scaffold types.},
  language = {en},
  number = {11},
  journal = {Journal of Chemical Information and Modeling},
  author = {Ruddigkeit, Lars and {van Deursen}, Ruud and Blum, Lorenz C. and Reymond, Jean-Louis},
  month = nov,
  year = {2012},
  pages = {2864-2875},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\PTQPW5X3\\ruddigkeit_et_al_2012_enumeration_of_166_billion_organic_small_molecules_in_the_chemical_universe.pdf}
}

@article{gilmer_neuralmessagepassing_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.01212},
  primaryClass = {cs},
  title = {Neural {{Message Passing}} for {{Quantum Chemistry}}},
  abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
  journal = {arXiv:1704.01212 [cs]},
  author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
  month = apr,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Computer Science - Learning,I.2.6},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\HEBYPZEV\\gilmer_et_al_2017_neural_message_passing_for_quantum_chemistry.pdf;C:\\Users\\yunsh782\\zotero\\storage\\LB3BYYY2\\gilmer_et_al_2017_neural_message_passing_for_quantum_chemistry.pdf}
}

@article{smith_lessmoresampling_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1801.09319},
  primaryClass = {physics, stat},
  title = {Less Is More: Sampling Chemical Space with Active Learning},
  shorttitle = {Less Is More},
  abstract = {The development of accurate and transferable machine learning (ML) potentials for predicting molecular energetics is a challenging task. The process of data generation to train such ML potentials is a task neither well understood nor researched in detail. In this work, we present a fully automated approach for the generation of datasets with the intent of training universal ML potentials. It is based on the concept of active learning (AL) via Query by Committee (QBC), which uses the disagreement between an ensemble of ML potentials to infer the reliability of the ensemble's prediction. QBC allows the presented AL algorithm to automatically sample regions of chemical space where the ML potential fails to accurately predict the potential energy. AL improves the overall fitness of ANAKIN-ME (ANI) deep learning potentials in rigorous test cases by mitigating human biases in deciding what new training data to use. AL also reduces the training set size to a fraction of the data required when using naive random sampling techniques. To provide validation of our AL approach we develop the COMP6 benchmark (publicly available on GitHub), which contains a diverse set of organic molecules. Through the AL process, it is shown that the AL-based potentials perform as well as the ANI-1 potential on COMP6 with only 10\% of the data, and vastly outperforms ANI-1 with 25\% the amount of data. Finally, we show that our proposed AL technique develops a universal ANI potential (ANI-1x) that provides accurate energy and force predictions on the entire COMP6 benchmark. This universal ML potential achieves a level of accuracy on par with the best ML potentials for single molecule or materials, while remaining applicable to the general class of organic molecules comprised of the elements CHNO.},
  journal = {arXiv:1801.09319 [physics, stat]},
  author = {Smith, Justin S. and Nebgen, Ben and Lubbers, Nicholas and Isayev, Olexandr and Roitberg, Adrian E.},
  month = jan,
  year = {2018},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\Z9I4WHNF\\smith_et_al_2018_less_is_more.pdf}
}

@article{kearnes_moleculargraphconvolutions_2016,
  title = {Molecular Graph Convolutions: Moving beyond Fingerprints},
  volume = {30},
  issn = {0920-654X, 1573-4951},
  shorttitle = {Molecular Graph Convolutions},
  doi = {10.1007/s10822-016-9938-8},
  abstract = {Molecular ``fingerprints'' encoding structural information are the workhorse of cheminformatics and machine learning in drug discovery applications. However, fingerprint representations necessarily emphasize particular aspects of the molecular structure while ignoring others, rather than allowing the model to make data-driven decisions. We describe molecular graph convolutions, a machine learning architecture for learning from undirected graphs, specifically small molecules. Graph convolutions use a simple encoding of the molecular graph\textemdash{}atoms, bonds, distances, etc.\textemdash{}which allows the model to take greater advantage of information in the graph structure. Although graph convolutions do not outperform all fingerprint-based methods, they (along with other graph-based methods) represent a new paradigm in ligand-based virtual screening with exciting opportunities for future improvement.},
  language = {en},
  number = {8},
  journal = {Journal of Computer-Aided Molecular Design},
  author = {Kearnes, Steven and McCloskey, Kevin and Berndl, Marc and Pande, Vijay and Riley, Patrick},
  month = aug,
  year = {2016},
  pages = {595-608},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\5GH9GRI9\\kearnes_et_al_2016_molecular_graph_convolutions.pdf}
}

@article{schutt_quantumchemicalinsightsdeep_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.08259},
  title = {Quantum-{{Chemical Insights}} from {{Deep Tensor Neural Networks}}},
  volume = {8},
  issn = {2041-1723},
  doi = {10.1038/ncomms13890},
  abstract = {Learning from data has led to paradigm shifts in a multitude of disciplines, including web, text, and image search, speech recognition, as well as bioinformatics. Can machine learning enable similar breakthroughs in understanding quantum many-body systems? Here we develop an efficient deep learning approach that enables spatially and chemically resolved insights into quantum-mechanical observables of molecular systems. We unify concepts from many-body Hamiltonians with purpose-designed deep tensor neural networks (DTNN), which leads to size-extensive and uniformly accurate (1 kcal/mol) predictions in compositional and configurational chemical space for molecules of intermediate size. As an example of chemical relevance, the DTNN model reveals a classification of aromatic rings with respect to their stability -- a useful property that is not contained as such in the training dataset. Further applications of DTNN for predicting atomic energies and local chemical potentials in molecules, reliable isomer energies, and molecules with peculiar electronic structure demonstrate the high potential of machine learning for revealing novel insights into complex quantum-chemical systems.},
  journal = {Nature Communications},
  author = {Sch\"utt, Kristof T. and Arbabzadah, Farhad and Chmiela, Stefan and M\"uller, Klaus R. and Tkatchenko, Alexandre},
  month = jan,
  year = {2017},
  keywords = {Physics - Chemical Physics},
  pages = {13890},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\WBI6FYQJ\\schütt_et_al_2017_quantum-chemical_insights_from_deep_tensor_neural_networks.pdf}
}

@article{yao_kineticenergyhydrocarbons_2016,
  title = {Kinetic {{Energy}} of {{Hydrocarbons}} as a {{Function}} of {{Electron Density}} and {{Convolutional Neural Networks}}},
  volume = {12},
  issn = {1549-9618},
  doi = {10.1021/acs.jctc.5b01011},
  abstract = {We demonstrate a convolutional neural network trained to reproduce the Kohn\textendash{}Sham kinetic energy of hydrocarbons from an input electron density. The output of the network is used as a nonlocal correction to conventional local and semilocal kinetic functionals. We show that this approximation qualitatively reproduces Kohn\textendash{}Sham potential energy surfaces when used with conventional exchange correlation functionals. The density which minimizes the total energy given by the functional is examined in detail. We identify several avenues to improve on this exploratory work, by reducing numerical noise and changing the structure of our functional. Finally we examine the features in the density learned by the neural network to anticipate the prospects of generalizing these models.},
  number = {3},
  journal = {Journal of Chemical Theory and Computation},
  author = {Yao, Kun and Parkhill, John},
  month = mar,
  year = {2016},
  pages = {1139-1147},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\45W38DPL\\yao_parkhill_2016_kinetic_energy_of_hydrocarbons_as_a_function_of_electron_density_and.pdf}
}

@article{faber_machinelearningprediction_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1702.05532},
  primaryClass = {physics},
  title = {Machine Learning Prediction Errors Better than {{DFT}} Accuracy},
  abstract = {We investigate the impact of choosing regressors and molecular representations for the construction of fast machine learning (ML) models of thirteen electronic ground-state properties of organic molecules. The performance of each regressor/representation/property combination is assessed using learning curves which report out-of-sample errors as a function of training set size with up to \$$\backslash$sim\$117k distinct molecules. Molecular structures and properties at hybrid density functional theory (DFT) level of theory used for training and testing come from the QM9 database [Ramakrishnan et al, \{$\backslash$em Scientific Data\} \{$\backslash$bf 1\} 140022 (2014)] and include dipole moment, polarizability, HOMO/LUMO energies and gap, electronic spatial extent, zero point vibrational energy, enthalpies and free energies of atomization, heat capacity and the highest fundamental vibrational frequency. Various representations from the literature have been studied (Coulomb matrix, bag of bonds, BAML and ECFP4, molecular graphs (MG)), as well as newly developed distribution based variants including histograms of distances (HD), and angles (HDA/MARAD), and dihedrals (HDAD). Regressors include linear models (Bayesian ridge regression (BR) and linear regression with elastic net regularization (EN)), random forest (RF), kernel ridge regression (KRR) and two types of neural net works, graph convolutions (GC) and gated graph networks (GG). We present numerical evidence that ML model predictions deviate from DFT less than DFT deviates from experiment for all properties. Furthermore, our out-of-sample prediction errors with respect to hybrid DFT reference are on par with, or close to, chemical accuracy. Our findings suggest that ML models could be more accurate than hybrid DFT if explicitly electron correlated quantum (or experimental) data was available.},
  journal = {arXiv:1702.05532 [physics]},
  author = {Faber, Felix A. and Hutchison, Luke and Huang, Bing and Gilmer, Justin and Schoenholz, Samuel S. and Dahl, George E. and Vinyals, Oriol and Kearnes, Steven and Riley, Patrick F. and {von Lilienfeld}, O. Anatole},
  month = feb,
  year = {2017},
  keywords = {Physics - Chemical Physics},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\E4Y7KUCS\\faber_et_al_2017_machine_learning_prediction_errors_better_than_dft_accuracy.pdf}
}

@article{collins_constantsizemolecular_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.06649},
  primaryClass = {physics, stat},
  title = {Constant {{Size Molecular Descriptors For Use With Machine Learning}}},
  abstract = {A set of molecular descriptors whose length is independent of molecular size is developed for machine learning models that target thermodynamic and electronic properties of molecules. These features are evaluated by monitoring performance of kernel ridge regression models on well-studied data sets of small organic molecules. The features include connectivity counts, which require only the bonding pattern of the molecule, and encoded distances, which summarize distances between both bonded and non-bonded atoms and so require the full molecular geometry. In addition to having constant size, these features summarize information regarding the local environment of atoms and bonds, such that models can take advantage of similarities resulting from the presence of similar chemical fragments across molecules. Combining these two types of features leads to models whose performance is comparable to or better than the current state of the art. The features introduced here have the advantage of leading to models that may be trained on smaller molecules and then used successfully on larger molecules.},
  journal = {arXiv:1701.06649 [physics, stat]},
  author = {Collins, Christopher R. and Gordon, Geoffrey J. and {von Lilienfeld}, O. Anatole and Yaron, David J.},
  month = jan,
  year = {2017},
  keywords = {Statistics - Machine Learning,Physics - Chemical Physics},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\JND9KGFH\\collins_et_al_2017_constant_size_molecular_descriptors_for_use_with_machine_learning.pdf}
}

@article{gao_modelingpalladiumsurfaces_2018,
  title = {Modeling Palladium Surfaces with Density Functional Theory, Neural Networks and Molecular Dynamics},
  issn = {0920-5861},
  doi = {10.1016/j.cattod.2018.03.045},
  abstract = {In this work, we have constructed a high dimensional neural network (NN) potential energy function for simulating palladium surface properties. The NN potential was trained with 3035 density functional theory (DFT) calculations, and was shown to be nearly as accurate as DFT in molecular simulations. Important properties including lattice constants, elastic properties and surface energies as well as transition state energies and adatom diffusion barriers were predicted by the NN and were found to be in excellent agreement with DFT results. The computational time to run the NN was compared to DFT calculation time, and we found this implementation of the NN is roughly four orders of magnitude faster than DFT. This approach is general and applicable to other systems and may have applications in modeling catalytic processes at surfaces.},
  journal = {Catalysis Today},
  author = {Gao, Tianyu and Kitchin, John R.},
  month = mar,
  year = {2018},
  keywords = {Molecular dynamics,Machine learning,Neural network,Potential energy surface},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\HIPVDC7U\\gao_kitchin_2018_modeling_palladium_surfaces_with_density_functional_theory,_neural_networks_and.pdf}
}

@article{bartokalbertp._gaussianapproximationpotentials_2015,
  title = {Gaussian Approximation Potentials: {{A}} Brief Tutorial Introduction},
  volume = {115},
  issn = {0020-7608},
  shorttitle = {Gaussian Approximation Potentials},
  doi = {10.1002/qua.24927},
  abstract = {We present a swift walk?through of our recent work that uses machine learning to fit interatomic potentials based on quantum mechanical data. We describe our Gaussian approximation potentials (GAP) framework, discuss a variety of descriptors, how to train the model on total energies and derivatives, and the simultaneous use of multiple models of different complexity. We also show a small example using QUIP, the software sandbox implementation of GAP that is available for noncommercial use. ? 2015 Wiley Periodicals, Inc.},
  number = {16},
  journal = {International Journal of Quantum Chemistry},
  author = {{Bart\'ok Albert P.} and {Cs\'anyi G\'abor}},
  month = apr,
  year = {2015},
  keywords = {ab initio,atomic environments,Gaussian process,interatomic potentials,machine learning},
  pages = {1051-1057},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\FNPE6RKQ\\bartók_albert_p._csányi_gábor_2015_gaussian_approximation_potentials.pdf}
}

@article{bartok_representingchemicalenvironments_2013,
  title = {On Representing Chemical Environments},
  volume = {87},
  doi = {10.1103/PhysRevB.87.184115},
  abstract = {We review some recently published methods to represent atomic neighborhood environments, and analyze their relative merits in terms of their faithfulness and suitability for fitting potential energy surfaces. The crucial properties that such representations (sometimes called descriptors) must have are differentiability with respect to moving the atoms and invariance to the basic symmetries of physics: rotation, reflection, translation, and permutation of atoms of the same species. We demonstrate that certain widely used descriptors that initially look quite different are specific cases of a general approach, in which a finite set of basis functions with increasing angular wave numbers are used to expand the atomic neighborhood density function. Using the example system of small clusters, we quantitatively show that this expansion needs to be carried to higher and higher wave numbers as the number of neighbors increases in order to obtain a faithful representation, and that variants of the descriptors converge at very different rates. We also propose an altogether different approach, called Smooth Overlap of Atomic Positions, that sidesteps these difficulties by directly defining the similarity between any two neighborhood environments, and show that it is still closely connected to the invariant descriptors. We test the performance of the various representations by fitting models to the potential energy surface of small silicon clusters and the bulk crystal.},
  number = {18},
  journal = {Physical Review B},
  author = {Bart\'ok, Albert P. and Kondor, Risi and Cs\'anyi, G\'abor},
  month = may,
  year = {2013},
  pages = {184115},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\S2KIWN6D\\bartók_et_al_2013_on_representing_chemical_environments.pdf}
}

@article{artrith_efficientaccuratemachinelearning_2017,
  title = {Efficient and Accurate Machine-Learning Interpolation of Atomic Energies in Compositions with Many Species},
  volume = {96},
  issn = {2469-9950, 2469-9969},
  doi = {10.1103/PhysRevB.96.014112},
  language = {en},
  number = {1},
  journal = {Physical Review B},
  author = {Artrith, Nongnuch and Urban, Alexander and Ceder, Gerbrand},
  month = jul,
  year = {2017},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\8DSBDYNX\\artrith_et_al_2017_efficient_and_accurate_machine-learning_interpolation_of_atomic_energies_in.pdf;C:\\Users\\yunsh782\\zotero\\storage\\KRBFBKRB\\si_artrith_et_al_2017_efficient_and_accurate_machine-learning_interpolation_of_atomic_energies_in.pdf}
}

@article{rupp_fastaccuratemodeling_2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1109.2618},
  title = {Fast and {{Accurate Modeling}} of {{Molecular Atomization Energies}} with {{Machine Learning}}},
  volume = {108},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.108.058301},
  abstract = {We introduce a machine learning model to predict atomization energies of a diverse set of organic molecules, based on nuclear charges and atomic positions only. The problem of solving the molecular Schr$\backslash$"odinger equation is mapped onto a non-linear statistical regression problem of reduced complexity. Regression models are trained on and compared to atomization energies computed with hybrid density-functional theory. Cross-validation over more than seven thousand small organic molecules yields a mean absolute error of \textasciitilde{}10 kcal/mol. Applicability is demonstrated for the prediction of molecular atomization potential energy curves.},
  number = {5},
  journal = {Physical Review Letters},
  author = {Rupp, Matthias and Tkatchenko, Alexandre and M\"uller, Klaus-Robert and {von Lilienfeld}, O. Anatole},
  month = jan,
  year = {2012},
  keywords = {Statistics - Machine Learning,Condensed Matter - Materials Science,Physics - Chemical Physics,Condensed Matter - Disordered Systems and Neural Networks},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\5LQM5VM9\\rupp_et_al_2012_fast_and_accurate_modeling_of_molecular_atomization_energies_with_machine.pdf}
}

@article{moussa_commentfastaccurate_2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1208.1085},
  title = {Comment on "{{Fast}} and {{Accurate Modeling}} of {{Molecular Atomization Energies}} with {{Machine Learning}}"},
  volume = {109},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.109.059801},
  abstract = {A Comment on the Letter by M. Rupp et al., Phys. Rev. Lett. 108 058301 (2012).},
  number = {5},
  journal = {Physical Review Letters},
  author = {Moussa, Jonathan E.},
  month = aug,
  year = {2012},
  keywords = {Condensed Matter - Materials Science,Physics - Chemical Physics},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\95B2NUF8\\moussa_2012_comment_on_fast_and_accurate_modeling_of_molecular_atomization_energies_with.pdf}
}

@article{hansen_machinelearningpredictions_2015,
  title = {Machine {{Learning Predictions}} of {{Molecular Properties}}: {{Accurate Many}}-{{Body Potentials}} and {{Nonlocality}} in {{Chemical Space}}},
  volume = {6},
  issn = {1948-7185},
  shorttitle = {Machine {{Learning Predictions}} of {{Molecular Properties}}},
  doi = {10.1021/acs.jpclett.5b00831},
  abstract = {Simultaneously accurate and efficient prediction of molecular properties throughout chemical compound space is a critical ingredient toward rational compound design in chemical and pharmaceutical industries. Aiming toward this goal, we develop and apply a systematic hierarchy of efficient empirical methods to estimate atomization and total energies of molecules. These methods range from a simple sum over atoms, to addition of bond energies, to pairwise interatomic force fields, reaching to the more sophisticated machine learning approaches that are capable of describing collective interactions between many atoms or bonds. In the case of equilibrium molecular geometries, even simple pairwise force fields demonstrate prediction accuracy comparable to benchmark energies calculated using density functional theory with hybrid exchange-correlation functionals; however, accounting for the collective many-body interactions proves to be essential for approaching the ``holy grail'' of chemical accuracy of 1 kcal/mol for both equilibrium and out-of-equilibrium geometries. This remarkable accuracy is achieved by a vectorized representation of molecules (so-called Bag of Bonds model) that exhibits strong nonlocality in chemical space. In addition, the same representation allows us to predict accurate electronic properties of molecules, such as their polarizability and molecular frontier orbital energies.},
  number = {12},
  journal = {The Journal of Physical Chemistry Letters},
  author = {Hansen, Katja and Biegler, Franziska and Ramakrishnan, Raghunathan and Pronobis, Wiktor and {von Lilienfeld}, O. Anatole and M\"uller, Klaus-Robert and Tkatchenko, Alexandre},
  month = jun,
  year = {2015},
  pages = {2326-2331},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\K5CLEEN2\\hansen_et_al_2015_machine_learning_predictions_of_molecular_properties.pdf}
}

@article{john_manybodycoarsegrainedinteractions_2017,
  title = {Many-{{Body Coarse}}-{{Grained Interactions Using Gaussian Approximation Potentials}}},
  volume = {121},
  issn = {1520-6106},
  doi = {10.1021/acs.jpcb.7b09636},
  abstract = {We introduce a computational framework that is able to describe general many-body coarse-grained (CG) interactions of molecules and use it to model the free energy surface of molecular liquids as a cluster expansion in terms of monomer, dimer, and trimer terms. The contributions to the free energy due to these terms are inferred from all-atom molecular dynamics (MD) data using Gaussian Approximation Potentials, a type of machine-learning model that employs Gaussian process regression. The resulting CG model is much more accurate than those possible using pair potentials. Though slower than the latter, our model can still be faster than all-atom simulations for solvent-free CG models commonly used in biomolecular simulations.},
  number = {48},
  journal = {The Journal of Physical Chemistry B},
  author = {John, S. T. and Cs\'anyi, G\'abor},
  month = dec,
  year = {2017},
  pages = {10934-10949},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\BR4N4F5D\\john_csányi_2017_many-body_coarse-grained_interactions_using_gaussian_approximation_potentials.pdf}
}

@article{gastegger_machinelearningmolecular_2017,
  title = {Machine Learning Molecular Dynamics for the Simulation of Infrared Spectra},
  volume = {8},
  issn = {2041-6520, 2041-6539},
  doi = {10.1039/C7SC02267K},
  language = {en},
  number = {10},
  journal = {Chemical Science},
  author = {Gastegger, Michael and Behler, J\"org and Marquetand, Philipp},
  year = {2017},
  pages = {6924-6935},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\NE9T3UPD\\gastegger_et_al_2017_machine_learning_molecular_dynamics_for_the_simulation_of_infrared_spectra.pdf}
}

@article{smith_ani1extensibleneural_2017,
  title = {{{ANI}}-1: An Extensible Neural Network Potential with {{DFT}} Accuracy at Force Field Computational Cost},
  volume = {8},
  issn = {2041-6520, 2041-6539},
  shorttitle = {{{ANI}}-1},
  doi = {10.1039/C6SC05720A},
  language = {en},
  number = {4},
  journal = {Chemical Science},
  author = {Smith, J. S. and Isayev, O. and Roitberg, A. E.},
  year = {2017},
  pages = {3192-3203},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\MZ4AASNN\\smith_et_al_2017_ani-1.pdf}
}

@article{yao_intrinsicbondenergies_2017,
  title = {Intrinsic {{Bond Energies}} from a {{Bonds}}-in-{{Molecules Neural Network}}},
  volume = {8},
  issn = {1948-7185},
  doi = {10.1021/acs.jpclett.7b01072},
  language = {en},
  number = {12},
  journal = {The Journal of Physical Chemistry Letters},
  author = {Yao, Kun and Herr, John E. and Brown, Seth N. and Parkhill, John},
  month = jun,
  year = {2017},
  pages = {2689-2694},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\QGXUALL8\\yao_et_al_2017_intrinsic_bond_energies_from_a_bonds-in-molecules_neural_network.pdf;C:\\Users\\yunsh782\\zotero\\storage\\TWE6RSU3\\yao_et_al_2017_intrinsic_bond_energies_from_a_bonds-in-molecules_neural_network.pdf}
}

@article{behler_constructinghighdimensionalneural_2015,
  title = {Constructing High-Dimensional Neural Network Potentials: {{A}} Tutorial Review},
  volume = {115},
  issn = {00207608},
  shorttitle = {Constructing High-Dimensional Neural Network Potentials},
  doi = {10.1002/qua.24890},
  language = {en},
  number = {16},
  journal = {International Journal of Quantum Chemistry},
  author = {Behler, J\"org},
  month = aug,
  year = {2015},
  pages = {1032-1050},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\X4V2FSJH\\behler_2015_constructing_high-dimensional_neural_network_potentials.pdf}
}

@article{behler_atomcenteredsymmetryfunctions_2011,
  title = {Atom-Centered Symmetry Functions for Constructing High-Dimensional Neural Network Potentials},
  volume = {134},
  issn = {0021-9606},
  doi = {10.1063/1.3553717},
  number = {7},
  journal = {The Journal of Chemical Physics},
  author = {Behler, J\"org},
  month = feb,
  year = {2011},
  pages = {074106},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\2MZ9RQNQ\\behler_2011_atom-centered_symmetry_functions_for_constructing_high-dimensional_neural.pdf}
}

@article{christensen_operatorsmachinelearning_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.08811},
  primaryClass = {physics},
  title = {Operators in {{Machine Learning}}: {{Response Properties}} in {{Chemical Space}}},
  shorttitle = {Operators in {{Machine Learning}}},
  abstract = {The role of response operators is well established in quantum mechanics. We investigate their use for universal quantum machine learning models of response properties in molecules. After introducing a theoretical basis, we present and discuss numerical evidence based on measuring the potential energy's response with respect to atomic displacement and to electric fields. Prediction errors for corresponding properties, atomic forces and dipole moments, improve in a systematic fashion with training set size and reach high accuracy for small training sets. Prediction of normal modes and IR-spectra of some small molecules demonstrates the usefulness of this approach for chemistry.},
  journal = {arXiv:1807.08811 [physics]},
  author = {Christensen, Anders S. and Faber, Felix A. and {von Lilienfeld}, O. Anatole},
  month = jul,
  year = {2018},
  keywords = {Physics - Chemical Physics},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\R6SYNUHQ\\christensen_et_al_2018_operators_in_machine_learning.pdf}
}

@article{huang_dnachemistryscalable_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.04146},
  primaryClass = {physics},
  title = {The "{{DNA}}" of Chemistry: {{Scalable}} Quantum Machine Learning with "Amons"},
  shorttitle = {The "{{DNA}}" of Chemistry},
  abstract = {Given sufficient examples, recently introduced machine learning models enable rapid, yet accurate, predictions of properties of new molecules. Extrapolation to larger molecules with differing composition is prohibitive due to all the specific chemistries which would be required for training. We address this problem by exploiting redundancies due to chemical similarity of repeating building blocks each represented by an effective \{$\backslash$underline a\}tom in \{$\backslash$underline m\}olecule: The "am-on". In analogy to the DNA sequence in a gene encoding its function, constituting amons encode a query molecule's properties. The use of amons affords highly accurate machine learning predictions of quantum properties of arbitrary query molecules in real time. We investigate this approach for predicting energies of various covalently and non-covalently bonded systems. After training on the few amons detected, very low prediction errors can be reached, on par with experimental uncertainty. Systems studied include two dozen large biomolecules, eleven thousand medium sized organic molecules, large common polymers, water clusters, doped \$h\$BN sheets, bulk silicon, and Watson-Crick DNA base pairs. Conceptually, the amons extend Mendeleev's table to account for the chemical environments of elements. They represent an important stepping stone to machine learning based virtual chemical space exploration campaigns.},
  journal = {arXiv:1707.04146 [physics]},
  author = {Huang, Bing and {von Lilienfeld}, O. Anatole},
  month = jul,
  year = {2017},
  keywords = {Physics - Chemical Physics},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\3CQ45GEH\\huang_von_lilienfeld_2017_the_dna_of_chemistry.pdf}
}

@article{chmiela_machinelearningaccurate_2017,
  title = {Machine Learning of Accurate Energy-Conserving Molecular Force Fields},
  volume = {3},
  issn = {2375-2548},
  doi = {10.1126/sciadv.1603015},
  language = {en},
  number = {5},
  journal = {Science Advances},
  author = {Chmiela, Stefan and Tkatchenko, Alexandre and Sauceda, Huziel E. and Poltavsky, Igor and Sch\"utt, Kristof T. and M\"uller, Klaus-Robert},
  month = may,
  year = {2017},
  pages = {e1603015},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\36Y6WHHF\\chmiela_et_al_2017_machine_learning_of_accurate_energy-conserving_molecular_force_fields.pdf}
}

@article{nitta_threedimensionalvectorvalued_2006,
  title = {Three-{{Dimensional Vector Valued Neural Network}} and Its {{Generalization Ability}}},
  abstract = {This letter introduces a novel neural network whose input and output signals, and threshold values are all 3-dimensional real-valued vectors and whose weights are all 3-dimensional orthogonal matrices, and the related back-propagation learning algorithm. The algorithm allows new spatial characteristics to be treated.},
  language = {en},
  journal = {Neural Information Processing},
  author = {Nitta, Tohru},
  year = {2006},
  pages = {6},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\YB3CMW3U\\nitta_2006_three-dimensional_vector_valued_neural_network_and_its_generalization_ability.pdf}
}

@article{glielmo_accurateinteratomicforce_2017,
  title = {Accurate Interatomic Force Fields via Machine Learning with Covariant Kernels},
  volume = {95},
  doi = {10.1103/PhysRevB.95.214302},
  abstract = {We present a novel scheme to accurately predict atomic forces as vector quantities, rather than sets of scalar components, by Gaussian process (GP) regression. This is based on matrix-valued kernel functions, on which we impose the requirements that the predicted force rotates with the target configuration and is independent of any rotations applied to the configuration database entries. We show that such covariant GP kernels can be obtained by integration over the elements of the rotation group SO(d) for the relevant dimensionality d. Remarkably, in specific cases the integration can be carried out analytically and yields a conservative force field that can be recast into a pair interaction form. Finally, we show that restricting the integration to a summation over the elements of a finite point group relevant to the target system is sufficient to recover an accurate GP. The accuracy of our kernels in predicting quantum-mechanical forces in real materials is investigated by tests on pure and defective Ni, Fe, and Si crystalline systems.},
  number = {21},
  journal = {Physical Review B},
  author = {Glielmo, Aldo and Sollich, Peter and De Vita, Alessandro},
  month = jun,
  year = {2017},
  pages = {214302},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\MQACG2J3\\glielmo_et_al_2017_accurate_interatomic_force_fields_via_machine_learning_with_covariant_kernels.pdf}
}

@article{faber_alchemicalstructuraldistribution_2018,
  title = {Alchemical and Structural Distribution Based Representation for Universal Quantum Machine Learning},
  volume = {148},
  issn = {0021-9606, 1089-7690},
  doi = {10.1063/1.5020710},
  language = {en},
  number = {24},
  journal = {The Journal of Chemical Physics},
  author = {Faber, Felix A. and Christensen, Anders S. and Huang, Bing and {von Lilienfeld}, O. Anatole},
  month = jun,
  year = {2018},
  pages = {241717},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\SCUFHQTU\\faber_et_al_2018_alchemical_and_structural_distribution_based_representation_for_universal.pdf}
}

@article{wang_forcefieldwater_2018,
  title = {Force {{Field}} for {{Water Based}} on {{Neural Network}}},
  volume = {9},
  issn = {1948-7185},
  doi = {10.1021/acs.jpclett.8b01131},
  abstract = {We developed a novel neural network-based force field for water based on training with high-level ab initio theory. The force field was built based on an electrostatically embedded many-body expansion method truncated at binary interactions. The many-body expansion method is a common strategy to partition the total Hamiltonian of large systems into a hierarchy of few-body terms. Neural networks were trained to represent electrostatically embedded one-body and two-body interactions, which require as input only one and two water molecule calculations at the level of ab initio electronic structure method CCSD/aug-cc-pVDZ embedded in the molecular mechanics water environment, making it efficient as a general force field construction approach. Structural and dynamic properties of liquid water calculated with our force field show good agreement with experimental results. We constructed two sets of neural network based force fields: nonpolarizable and polarizable force fields. Simulation results show that the nonpolarizable force field using fixed TIP3P charges has already behaved well, since polarization effects and many-body effects are implicitly included due to the electrostatic embedding scheme. Our results demonstrate that the electrostatically embedded many-body expansion combined with neural network provides a promising and systematic way to build next-generation force fields at high accuracy and low computational costs, especially for large systems.},
  language = {en},
  number = {12},
  journal = {The Journal of Physical Chemistry Letters},
  author = {Wang, Hao and Yang, Weitao},
  month = jun,
  year = {2018},
  pages = {3232-3240},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\M3XQFH8V\\wang_yang_2018_force_field_for_water_based_on_neural_network.pdf}
}

@incollection{kashima_pairwisekernelsefficient_2009,
  address = {Berlin, Heidelberg},
  title = {On {{Pairwise Kernels}}: {{An Efficient Alternative}} and {{Generalization Analysis}}},
  volume = {5476},
  isbn = {978-3-642-01306-5 978-3-642-01307-2},
  shorttitle = {On {{Pairwise Kernels}}},
  abstract = {Pairwise classification has many applications including network prediction, entity resolution, and collaborative filtering. The pairwise kernel has been proposed for those purposes by several research groups independently, and become successful in various fields. In this paper, we propose an efficient alternative which we call Cartesian kernel. While the existing pairwise kernel (which we refer to as Kronecker kernel) can be interpreted as the weighted adjacency matrix of the Kronecker product graph of two graphs, the Cartesian kernel can be interpreted as that of the Cartesian graph which is more sparse than the Kronecker product graph. Experimental results show the Cartesian kernel is much faster than the existing pairwise kernel, and at the same time, competitive with the existing pairwise kernel in predictive performance. We discuss the generalization bounds by the two pairwise kernels by using eigenvalue analysis of the kernel matrices.},
  language = {en},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Kashima, Hisashi and Oyama, Satoshi and Yamanishi, Yoshihiro and Tsuda, Koji},
  editor = {Theeramunkong, Thanaruk and Kijsirikul, Boonserm and Cercone, Nick and Ho, Tu-Bao},
  year = {2009},
  pages = {1030-1037},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\9VAFB93Y\\kashima_et_al_2009_on_pairwise_kernels.pdf},
  doi = {10.1007/978-3-642-01307-2_110}
}

@article{yao_manybodyexpansioncombined_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.07072},
  title = {The {{Many}}-{{Body Expansion Combined}} with {{Neural Networks}}},
  volume = {146},
  issn = {0021-9606, 1089-7690},
  doi = {10.1063/1.4973380},
  abstract = {Fragmentation methods such as the many-body expansion (MBE) are a common strategy to model large systems by partitioning energies into a hierarchy of decreasingly significant contributions. The number of fragments required for chemical accuracy is still prohibitively expensive for ab-initio MBE to compete with force field approximations for applications beyond single-point energies. Alongside the MBE, empirical models of ab-initio potential energy surfaces have improved, especially non-linear models based on neural networks (NN) which can reproduce ab-initio potential energy surfaces rapidly and accurately. Although they are fast, NNs suffer from their own curse of dimensionality; they must be trained on a representative sample of chemical space. In this paper we examine the synergy of the MBE and NN's, and explore their complementarity. The MBE offers a systematic way to treat systems of arbitrary size and intelligently sample chemical space. NN's reduce, by a factor in excess of \$10\^6\$ the computational overhead of the MBE and reproduce the accuracy of ab-initio calculations without specialized force fields. We show they are remarkably general, providing comparable accuracy with drastically different chemical embeddings. To assess this we test a new chemical embedding which can be inverted to predict molecules with desired properties.},
  number = {1},
  journal = {The Journal of Chemical Physics},
  author = {Yao, Kun and Herr, John E. and Parkhill, John},
  month = jan,
  year = {2017},
  keywords = {Statistics - Machine Learning,Physics - Computational Physics,Physics - Chemical Physics},
  pages = {014106},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\HDBNNJ46\\yao_et_al_2017_the_many-body_expansion_combined_with_neural_networks.pdf}
}

@article{chang_fastaccuratepredictions_2016,
  title = {Fast and Accurate Predictions of Covalent Bonds in Chemical Space},
  volume = {144},
  issn = {0021-9606},
  doi = {10.1063/1.4947217},
  number = {17},
  journal = {The Journal of Chemical Physics},
  author = {Chang, K. Y. Samuel and Fias, Stijn and Ramakrishnan, Raghunathan and {von Lilienfeld}, O. Anatole},
  month = may,
  year = {2016},
  pages = {174110},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\QK2CGPD9\\chang_et_al_2016_fast_and_accurate_predictions_of_covalent_bonds_in_chemical_space.pdf}
}

@article{morawietz_efficientsimulationswater_,
  title = {{Efficient simulations of water with ab initio accuracy : development of high-dimensional neural network potentials for water clusters and bulk water}},
  language = {de},
  author = {Morawietz, Tobias},
  pages = {192},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\95FHU34Q\\morawietz_efficient_simulations_of_water_with_ab_initio_accuracy.pdf}
}

@article{csanyi_learnflyhybrid_2004,
  title = {``{{Learn}} on the {{Fly}}'': {{A Hybrid Classical}} and {{Quantum}}-{{Mechanical Molecular Dynamics Simulation}}},
  volume = {93},
  issn = {0031-9007, 1079-7114},
  shorttitle = {``{{Learn}} on the {{Fly}}''},
  doi = {10.1103/PhysRevLett.93.175503},
  language = {en},
  number = {17},
  journal = {Physical Review Letters},
  author = {Cs\'anyi, Gabor and Albaret, T. and Payne, M. C. and De Vita, A.},
  month = oct,
  year = {2004},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\XW4TIZGV\\csányi_et_al_2004_“learn_on_the_fly”.pdf}
}

@article{li_moleculardynamicsonthefly_2015,
  title = {Molecular {{Dynamics}} with {{On}}-the-{{Fly Machine Learning}} of {{Quantum}}-{{Mechanical Forces}}},
  volume = {114},
  doi = {10.1103/PhysRevLett.114.096405},
  abstract = {We present a molecular dynamics scheme which combines first-principles and machine-learning (ML) techniques in a single information-efficient approach. Forces on atoms are either predicted by Bayesian inference or, if necessary, computed by on-the-fly quantum-mechanical (QM) calculations and added to a growing ML database, whose completeness is, thus, never required. As a result, the scheme is accurate and general, while progressively fewer QM calls are needed when a new chemical process is encountered for the second and subsequent times, as demonstrated by tests on crystalline and molten silicon.},
  number = {9},
  journal = {Physical Review Letters},
  author = {Li, Zhenwei and Kermode, James R. and De Vita, Alessandro},
  month = mar,
  year = {2015},
  pages = {096405},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\AHHC9GWR\\li_et_al_2015_molecular_dynamics_with_on-the-fly_machine_learning_of_quantum-mechanical_forces.pdf}
}

@article{alvarez_kernelsvectorvaluedfunctions_2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1106.6251},
  primaryClass = {cs, math, stat},
  title = {Kernels for {{Vector}}-{{Valued Functions}}: A {{Review}}},
  shorttitle = {Kernels for {{Vector}}-{{Valued Functions}}},
  abstract = {Kernel methods are among the most popular techniques in machine learning. From a frequentist/discriminative perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a Bayesian/generative perspective they are the key in the context of Gaussian processes, where the kernel function is also known as the covariance function. Traditionally, kernel methods have been used in supervised learning problem with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partly by frameworks like multitask learning. In this paper, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods.},
  journal = {arXiv:1106.6251 [cs, math, stat]},
  author = {Alvarez, Mauricio A. and Rosasco, Lorenzo and Lawrence, Neil D.},
  month = jun,
  year = {2011},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Mathematics - Statistics Theory},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\C72VCRBW\\alvarez_et_al_2011_kernels_for_vector-valued_functions.pdf}
}

@article{todorovic_efficientbayesianinference_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1708.09274},
  primaryClass = {cond-mat},
  title = {Efficient {{Bayesian Inference}} of {{Atomistic Structure}} in {{Complex Functional Materials}}},
  abstract = {Tailoring the functional properties of advanced organic/inorganic heterogeonous devices to their intended technological applications requires knowledge and control of the microscopic structure inside the device. Atomistic quantum mechanical simulation methods deliver accurate energies and properties for individual configurations, however, finding the most favourable configurations remains computationally prohibitive. We propose a 'building block'-based Bayesian Optimisation Structure Search (BOSS) approach for addressing extended organic/inorganic interface problems and demonstrate its feasibility in a molecular surface adsorption study. In BOSS, a likelihood-free Bayesian scheme accelerates the identification of material energy landscapes with the number of sampled configurations during active learning, enabling structural inference with high chemical accuracy and featuring large simulation cells. This allowed us to identify several most favourable molecular adsorption configurations for \$$\backslash$mathrm\{C\}\_\{60\}\$ on the (101) surface of \$$\backslash$mathrm\{TiO\}\_2\$ anatase and clarify the key molecule-surface interactions governing structural assembly. Inferred structures were in good agreement with detailed experimental images of this surface adsorbate, demonstrating good predictive power of BOSS and opening the route towards large-scale surface adsorption studies of molecular aggregates and films.},
  journal = {arXiv:1708.09274 [cond-mat]},
  author = {Todorovi\'c, Milica and Gutmann, Michael U. and Corander, Jukka and Rinke, Patrick},
  month = aug,
  year = {2017},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\SHE3A6RK\\todorović_et_al_2017_efficient_bayesian_inference_of_atomistic_structure_in_complex_functional.pdf}
}

@article{ghosh_deeplearningpredicting_2017,
  title = {Deep {{Learning}} for {{Predicting Molecular Electronic Properties}}},
  language = {en},
  author = {Ghosh, Kunal},
  year = {2017},
  pages = {46},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\MIZNZGRQ\\ghosh_deep_learning_for_predicting_molecular_electronic_properties.pdf}
}

@article{chulhai_projectionbasedcorrelatedwave_2018,
  title = {Projection-{{Based Correlated Wave Function}} in {{Density Functional Theory Embedding}} for {{Periodic Systems}}},
  volume = {14},
  issn = {1549-9618, 1549-9626},
  doi = {10.1021/acs.jctc.7b01154},
  abstract = {We present a level shift projection operatorbased embedding method for systems with periodic boundary conditionswhere the ``active'' subsystem can be described using either density functional theory (DFT) or correlated wave function (WF) methods and the ``environment'' is described using DFT. Our method allows for k-point sampling, is shown to be exactly equal to the canonical DFT solution of the full system under the limit that we use the full system basis to describe each subsystem, and can treat the active subsystem either with periodic boundary conditionsin what we term ``periodic-in-periodic'' embeddingor as a molecular clusterin ``cluster-in-periodic'' embedding. We explore each of these methods and show that cluster WF-in-periodic DFT embedding can accurately calculate the absorption energy of CO on to a Si(100)-2\texttimes{}1 surface.},
  language = {en},
  number = {4},
  journal = {Journal of Chemical Theory and Computation},
  author = {Chulhai, Dhabih V. and Goodpaster, Jason D.},
  month = apr,
  year = {2018},
  pages = {1928-1942},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\NAHE7JRB\\Chulhai and Goodpaster - 2018 - Projection-Based Correlated Wave Function in Densi.pdf}
}

@article{mocanu_modelingphasechangememory_2018,
  title = {Modeling the {{Phase}}-{{Change Memory Material}}, {{Ge}} {\textsubscript{2}} {{Sb}} {\textsubscript{2}} {{Te}} {\textsubscript{5}} , with a {{Machine}}-{{Learned Interatomic Potential}}},
  volume = {122},
  issn = {1520-6106, 1520-5207},
  doi = {10.1021/acs.jpcb.8b06476},
  abstract = {The phase-change material, Ge2Sb2Te5, is the canonical material ingredient for next-generation storage-class memory devices used in novel computing architectures, but fundamental questions remain regarding its atomic structure and physicochemical properties. Here, we introduce a machine-learning (ML)-based interatomic potential that enables large-scale atomistic simulations of liquid, amorphous, and crystalline Ge2Sb2Te5 with an unprecedented combination of speed and density functional theory (DFT) level of accuracy. Two applications exemplify the usefulness of such an ML-driven approach: we generate a 7200-atom structural model, hitherto inaccessible with DFT simulations, that affords new insight into the medium-range structural order and we create an ensemble of uncorrelated, smaller structures, for studies of their chemical bonding with statistical significance. Our work opens the way for new atomistic insights into the fascinating and chemically complex class of phase-change materials that are used in real nonvolatile memory devices.},
  language = {en},
  number = {38},
  journal = {The Journal of Physical Chemistry B},
  author = {Mocanu, Felix C. and Konstantinou, Konstantinos and Lee, Tae Hoon and Bernstein, Noam and Deringer, Volker L. and Cs\'anyi, G\'abor and Elliott, Stephen R.},
  month = sep,
  year = {2018},
  pages = {8998-9006},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\ZUNST8WT\\mocanu_et_al_2018_modeling_the_phase-change_memory_material,_ge_sub2-sub_sb_sub2-sub_te.pdf}
}

@article{mcdonagh_machinelearningdynamic_2018,
  title = {Machine {{Learning}} of {{Dynamic Electron Correlation Energies}} from {{Topological Atoms}}},
  volume = {14},
  issn = {1549-9618, 1549-9626},
  doi = {10.1021/acs.jctc.7b01157},
  abstract = {We present an innovative method for predicting the dynamic electron correlation energy of an atom or a bond in a molecule utilizing topological atoms. Our approach uses the machine learning method Kriging (Gaussian Process Regression with a non-zero mean function) to predict these dynamic electron correlation energy contributions. The true energy values are calculated by partitioning the MP2 two-particle density-matrix via the Interacting Quantum Atoms (IQA) procedure. To our knowledge, this is the first time such energies have been predicted by a machine learning technique. We present here three important proof-of-concept cases: the water monomer, the water dimer, and the van der Waals complex H2$\cdot\cdot\cdot$He. These cases represent the final step toward the design of a full IQA potential for molecular simulation. This final piece will enable us to consider situations in which dispersion is the dominant intermolecular interaction. The results from these examples suggest a new method by which dispersion potentials for molecular simulation can be generated.},
  language = {en},
  number = {1},
  journal = {Journal of Chemical Theory and Computation},
  author = {McDonagh, James L. and Silva, Arnaldo F. and Vincent, Mark A. and Popelier, Paul L. A.},
  month = jan,
  year = {2018},
  pages = {216-224},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\46DUPDVN\\mcdonagh_et_al_2018_machine_learning_of_dynamic_electron_correlation_energies_from_topological_atoms.pdf}
}

@article{ryu_deeplylearningmolecular_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.10988},
  primaryClass = {cs, stat},
  title = {Deeply Learning Molecular Structure-Property Relationships Using Attention- and Gate-Augmented Graph Convolutional Network},
  abstract = {Molecular structure-property relationships are key to molecular engineering for materials and drug discovery. The rise of deep learning offers a new viable solution to elucidate the structure-property relationships directly from chemical data. Here we show that the performance of graph convolutional networks (GCNs) for the prediction of molecular properties can be improved by incorporating attention and gate mechanisms. The attention mechanism enables a GCN to identify atoms in different environments. The gated skip-connection further improves the GCN by updating feature maps at an appropriate rate. We demonstrate that the resulting attention- and gate-augmented GCN could extract better structural features related to a target molecular property such as solubility, polarity, synthetic accessibility and photovoltaic efficiency compared to the vanilla GCN. More interestingly, it identified two distinct parts of molecules as essential structural features for high photovoltaic efficiency, and each of them coincided with the areas of donor and acceptor orbitals for charge-transfer excitations, respectively. As a result, the new model could accurately predict molecular properties and place molecules with similar properties close to each other in a well-trained latent space, which is critical for successful molecular engineering.},
  journal = {arXiv:1805.10988 [cs, stat]},
  author = {Ryu, Seongok and Lim, Jaechang and Hong, Seung Hwan and Kim, Woo Youn},
  month = may,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\QR4QLJ4Y\\ryu_et_al_2018_deeply_learning_molecular_structure-property_relationships_using_attention-_and.pdf}
}

@article{duvenaud_convolutionalnetworksgraphs_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.09292},
  primaryClass = {cs, stat},
  title = {Convolutional {{Networks}} on {{Graphs}} for {{Learning Molecular Fingerprints}}},
  abstract = {We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.},
  journal = {arXiv:1509.09292 [cs, stat]},
  author = {Duvenaud, David and Maclaurin, Dougal and {Aguilera-Iparraguirre}, Jorge and {G\'omez-Bombarelli}, Rafael and Hirzel, Timothy and {Aspuru-Guzik}, Al\'an and Adams, Ryan P.},
  month = sep,
  year = {2015},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\MBACCWR3\\duvenaud_et_al_2015_convolutional_networks_on_graphs_for_learning_molecular_fingerprints.pdf}
}

@article{blanco_interactingquantumatoms_2005,
  title = {Interacting {{Quantum Atoms}}: {{A Correlated Energy Decomposition Scheme Based}} on the {{Quantum Theory}} of {{Atoms}} in {{Molecules}}},
  volume = {1},
  issn = {1549-9618, 1549-9626},
  shorttitle = {Interacting {{Quantum Atoms}}},
  doi = {10.1021/ct0501093},
  abstract = {We make use of the Quantum Theory of Atoms in Molecules (QTAM) to partition the total energy of a many-electron system into intra- and interatomic terms, by explicitly computing both the one- and two-electron contributions. While the general scheme is formally equivalent to that by Bader et al., we focus on the separation and computation of the atomic self-energies and all the interaction terms. The partition is ultimately performed within the density matrices, in analogy with McWeeny's Theory of Electronic Separability, and then carried onto the energy. It is intimately linked with the atomistic picture of the chemical bond, not only allowing the separation of different two-body contributions (point-charge-like, multipolar, total Coulomb, exchange, correlation, ...) to the interaction between a pair of atoms but also including an effective many-body contribution to the binding (self-energy, formally one-body) due to the deformation of the atoms within the many-electron system as compared to the free atoms. Many qualitative ideas about the chemical bond can be quantified using this scheme.},
  language = {en},
  number = {6},
  journal = {Journal of Chemical Theory and Computation},
  author = {Blanco, M. A. and Mart\'in Pend\'as, A. and Francisco, E.},
  month = nov,
  year = {2005},
  pages = {1096-1109},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\WNR5H2W4\\blanco_et_al_2005_interacting_quantum_atoms_-_a_correlated_energy_decomposition_scheme_based_on.pdf}
}

@article{pankajakshan_machinelearningstatistical_2017,
  title = {Machine {{Learning}} and {{Statistical Analysis}} for {{Materials Science}}: {{Stability}} and {{Transferability}} of {{Fingerprint Descriptors}} and {{Chemical Insights}}},
  volume = {29},
  issn = {0897-4756, 1520-5002},
  shorttitle = {Machine {{Learning}} and {{Statistical Analysis}} for {{Materials Science}}},
  doi = {10.1021/acs.chemmater.6b04229},
  abstract = {In the paradigm of virtual high-throughput screening for materials, we have developed a semiautomated workflow or ``recipe'' that can help a material scientist to start from a raw data set of materials with their properties and descriptors, build predictive models, and draw insights into the governing mechanism. We demonstrate our recipe, which employs machine learning tools and statistical analysis, through application to a case study leading to identification of descriptors relevant to catalysts for CO2 electroreduction, starting from a published database of 298 catalyst alloys. At the heart of our methodology lies the Bootstrapped Projected Gradient Descent (BoPGD) algorithm, which has significant advantages over commonly used machine learning (ML) and statistical analysis (SA) tools such as the regression coefficient shrinkage-based method (LASSO) or artificial neural networks: (a) it selects descriptors with greater stability and transferability, with a goal to understand the chemical mechanism rather than fitting data, and (b) while being effective for smaller data sets such as in the test case, it employs clustering of descriptors to scale far more efficiently to large size of descriptor sets in terms of computational speed. In addition to identifying the descriptors that parametrize the d-band model of catalysts for CO2 reduction, we predict work function to be an essential and relevant descriptor. Based on this result, we propose a modification of the d-band model that includes the chemical effect of work function, and show that the resulting predictive model gives the binding energy of CO to catalyst fairly accurately. Since our scheme is general and particularly efficient in reducing a set of large number of descriptors to a minimal one, we expect it to be a versatile tool in obtaining chemical insights into complex phenomena and development of predictive models for design of materials.},
  language = {en},
  number = {10},
  journal = {Chemistry of Materials},
  author = {Pankajakshan, Praveen and Sanyal, Suchismita and {de Noord}, Onno E. and Bhattacharya, Indranil and Bhattacharyya, Arnab and Waghmare, Umesh},
  month = may,
  year = {2017},
  pages = {4190-4201},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\4V5WDSK4\\pankajakshan_et_al_2017_machine_learning_and_statistical_analysis_for_materials_science.pdf}
}

@article{butler_machinelearningmolecular_2018,
  title = {Machine Learning for Molecular and Materials Science},
  volume = {559},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-018-0337-2},
  language = {en},
  number = {7715},
  journal = {Nature},
  author = {Butler, Keith T. and Davies, Daniel W. and Cartwright, Hugh and Isayev, Olexandr and Walsh, Aron},
  month = jul,
  year = {2018},
  pages = {547-555},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\5EXQZJWB\\butler_et_al_2018_machine_learning_for_molecular_and_materials_science.pdf}
}

@article{drautz_atomicclusterexpansion_2019,
  title = {Atomic Cluster Expansion for Accurate and Transferable Interatomic Potentials},
  volume = {99},
  issn = {2469-9950, 2469-9969},
  doi = {10.1103/PhysRevB.99.014104},
  language = {en},
  number = {1},
  journal = {Physical Review B},
  author = {Drautz, Ralf},
  month = jan,
  year = {2019},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\KYISPK77\\drautz_2019_atomic_cluster_expansion_for_accurate_and_transferable_interatomic_potentials.pdf}
}

@article{senftle_reaxffreactiveforcefield_2016,
  title = {The {{ReaxFF}} Reactive Force-Field: Development, Applications and Future Directions},
  volume = {2},
  copyright = {2016 Nature Publishing Group},
  issn = {2057-3960},
  shorttitle = {The {{ReaxFF}} Reactive Force-Field},
  doi = {10.1038/npjcompumats.2015.11},
  abstract = {The reactive force-field (ReaxFF) interatomic potential is a powerful computational tool for exploring, developing and optimizing material properties. Methods based on the principles of quantum mechanics (QM), while offering valuable theoretical guidance at the electronic level, are often too computationally intense for simulations that consider the full dynamic evolution of a system. Alternatively, empirical interatomic potentials that are based on classical principles require significantly fewer computational resources, which enables simulations to better describe dynamic processes over longer timeframes and on larger scales. Such methods, however, typically require a predefined connectivity between atoms, precluding simulations that involve reactive events. The ReaxFF method was developed to help bridge this gap. Approaching the gap from the classical side, ReaxFF casts the empirical interatomic potential within a bond-order formalism, thus implicitly describing chemical bonding without expensive QM calculations. This article provides an overview of the development, application, and future directions of the ReaxFF method.},
  language = {en},
  journal = {npj Computational Materials},
  author = {Senftle, Thomas P. and Hong, Sungwook and Islam, Md Mahbubul and Kylasa, Sudhir B. and Zheng, Yuanxia and Shin, Yun Kyung and Junkermeier, Chad and {Engel-Herbert}, Roman and Janik, Michael J. and Aktulga, Hasan Metin and Verstraelen, Toon and Grama, Ananth and {van Duin}, Adri C. T.},
  month = mar,
  year = {2016},
  pages = {15011},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\744UQF3Y\\senftle_et_al_2016_the_reaxff_reactive_force-field.pdf}
}

@article{zubatyuk_accuratetransferablemultitask_2018,
  title = {Accurate and {{Transferable Multitask Prediction}} of {{Chemical Properties}} with an {{Atoms}}-in-{{Molecule Neural Network}}},
  doi = {10.26434/chemrxiv.7151435.v2},
  abstract = {Atomic and molecular properties could be evaluated from the
fundamental Schrodinger's equation and therefore represent different modalities
of the same quantum phenomena. Here we present AIMNet, a modular and chemically
inspired deep neural network potential. We used AIMNet with multitarget
training to learn multiple modalities of the state of the atom in a molecular
system. The resulting model shows on several benchmark datasets the state-of-the-art
accuracy, comparable to the results of orders of magnitude more expensive DFT
methods. It can simultaneously predict several atomic and molecular properties
without an increase in computational cost. With AIMNet we show a new dimension
of transferability: the ability to learn new targets utilizing multimodal
information from previous training. The model can learn implicit solvation
energy (like SMD) utilizing only a fraction of original training data, and
archive MAD error of 1.1 kcal/mol compared to experimental solvation free
energies in MNSol database.},
  language = {en},
  author = {Zubatyuk, Roman and Smith, Justin S. and Leszczynski, Jerzy and Isayev, Olexandr},
  month = oct,
  year = {2018}
}

@article{gastegger_wacsfweightedatomcentered_2018,
  title = {{{wACSF}}\textemdash{{Weighted}} Atom-Centered Symmetry Functions as Descriptors in Machine Learning Potentials},
  volume = {148},
  issn = {0021-9606},
  doi = {10.1063/1.5019667},
  abstract = {We introduce weighted atom-centered symmetry functions (wACSFs) as descriptors of a chemical system's geometry for use in the prediction of chemical properties such as enthalpies or potential energies via machine learning. The wACSFs are based on conventional atom-centered symmetry functions (ACSFs) but overcome the undesirable scaling of the latter with an increasing number of different elements in a chemical system. The performance of these two descriptors is compared using them as inputs in high-dimensional neural network potentials (HDNNPs), employing the molecular structures and associated enthalpies of the 133 855 molecules containing up to five different elements reported in the QM9 database as reference data. A substantially smaller number of wACSFs than ACSFs is needed to obtain a comparable spatial resolution of the molecular structures. At the same time, this smaller set of wACSFs leads to a significantly better generalization performance in the machine learning potential than the large set of conventional ACSFs. Furthermore, we show that the intrinsic parameters of the descriptors can in principle be optimized with a genetic algorithm in a highly automated manner. For the wACSFs employed here, we find however that using a simple empirical parametrization scheme is sufficient in order to obtain HDNNPs with high accuracy.},
  number = {24},
  journal = {The Journal of Chemical Physics},
  author = {Gastegger, M. and Schwiedrzik, L. and Bittermann, M. and Berzsenyi, F. and Marquetand, P.},
  month = mar,
  year = {2018},
  pages = {241709},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\YNK7VZ85\\gastegger_et_al_2018_wacsf—weighted_atom-centered_symmetry_functions_as_descriptors_in_machine.pdf}
}

@article{chuang_commentpredictingreaction_2018,
  title = {Comment on ``{{Predicting}} Reaction Performance in {{C}}\textendash{{N}} Cross-Coupling Using Machine Learning''},
  volume = {362},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aat8603},
  language = {en},
  number = {6416},
  journal = {Science},
  author = {Chuang, Kangway V. and Keiser, Michael J.},
  month = nov,
  year = {2018},
  pages = {eaat8603},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\EHC9HI6H\\chuang_keiser_2018_comment_on_“predicting_reaction_performance_in_c–n_cross-coupling_using_machine.pdf}
}

@article{ahneman_predictingreactionperformance_2018,
  title = {Predicting Reaction Performance in {{C}}\textendash{{N}} Cross-Coupling Using Machine Learning},
  volume = {360},
  copyright = {Copyright \textcopyright{} 2018 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aar5169},
  abstract = {A guide for catalyst choice in the forest
Chemists often discover reactions by applying catalysts to a series of simple compounds. Tweaking those reactions to tolerate more structural complexity in pharmaceutical research is time-consuming. Ahneman et al. report that machine learning can help. Using a high-throughput data set, they trained a random forest algorithm to predict which specific palladium catalysts would best tolerate isoxazoles (cyclic structures with an N\textendash{}O bond) during C\textendash{}N bond formation. The predictions also helped to guide analysis of the catalyst inhibition mechanism.
Science, this issue p. 186
Machine learning methods are becoming integral to scientific inquiry in numerous disciplines. We demonstrated that machine learning can be used to predict the performance of a synthetic reaction in multidimensional chemical space using data obtained via high-throughput experimentation. We created scripts to compute and extract atomic, molecular, and vibrational descriptors for the components of a palladium-catalyzed Buchwald-Hartwig cross-coupling of aryl halides with 4-methylaniline in the presence of various potentially inhibitory additives. Using these descriptors as inputs and reaction yield as output, we showed that a random forest algorithm provides significantly improved predictive performance over linear regression analysis. The random forest model was also successfully applied to sparse training sets and out-of-sample prediction, suggesting its value in facilitating adoption of synthetic methodology.
A random forest algorithm trained on high-throughput data predicts which catalysts best tolerate certain heterocycles.
A random forest algorithm trained on high-throughput data predicts which catalysts best tolerate certain heterocycles.},
  language = {en},
  number = {6385},
  journal = {Science},
  author = {Ahneman, Derek T. and Estrada, Jes\'us G. and Lin, Shishi and Dreher, Spencer D. and Doyle, Abigail G.},
  month = apr,
  year = {2018},
  pages = {186-190},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\ABM6JXEB\\ahneman_et_al_2018_predicting_reaction_performance_in_c–n_cross-coupling_using_machine_learning.pdf},
  pmid = {29449509}
}

@article{estrada_responsecommentpredicting_2018,
  title = {Response to {{Comment}} on ``{{Predicting}} Reaction Performance in {{C}}\textendash{{N}} Cross-Coupling Using Machine Learning''},
  volume = {362},
  copyright = {Copyright \textcopyright{} 2018, American Association for the Advancement of Science. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aat8763},
  abstract = {We demonstrate that the chemical-feature model described in our original paper is distinguishable from the nongeneralizable models introduced by Chuang and Keiser. Furthermore, the chemical-feature model significantly outperforms these models in out-of-sample predictions, justifying the use of chemical featurization from which machine learning models can extract meaningful patterns in the dataset, as originally described.},
  language = {en},
  number = {6416},
  journal = {Science},
  author = {Estrada, Jes\'us G. and Ahneman, Derek T. and Sheridan, Robert P. and Dreher, Spencer D. and Doyle, Abigail G.},
  month = nov,
  year = {2018},
  pages = {eaat8763},
  file = {C:\\Users\\yunsh782\\zotero\\storage\\XTKSMGEX\\estrada_et_al_2018_response_to_comment_on_“predicting_reaction_performance_in_c–n_cross-coupling.pdf},
  pmid = {30442777}
}


